from __future__ import annotations

import logging
import time

import pandas as pd
from autogluon.common.utils.try_import import try_import_xgboost
from autogluon.core.constants import PROBLEM_TYPES_CLASSIFICATION

logger = logging.getLogger(__name__)

from autogluon.tabular.models.xgboost.xgboost_model import XGBoostModel

from tabarena.tabarena.tabarena.benchmark.models.prep_ag.prep_mixin import ModelAgnosticPrepMixin

logger = logging.getLogger(__name__)

class PrepXGBoostModel(ModelAgnosticPrepMixin, XGBoostModel):
    """
    XGBoost model: https://xgboost.readthedocs.io/en/latest/

    Hyperparameter options: https://xgboost.readthedocs.io/en/latest/parameter.html
    """
    ag_key = "prep_XGB"
    ag_name = "prep_XGBoost"
    ag_priority = 40
    seed_name = "seed"

    def _fit(self, X, y, X_val=None, y_val=None, time_limit=None, num_gpus=0, num_cpus=None, sample_weight=None, sample_weight_val=None, verbosity=2, **kwargs):
        # TODO: utilize sample_weight_val in early-stopping if provided
        start_time = time.time()
        ag_params = self._get_ag_params()
        params = self._get_model_params()
        generate_curves = ag_params.get("generate_curves", False)

        if generate_curves:
            X_test = kwargs.get("X_test", None)
            y_test = kwargs.get("y_test", None)
        else:
            X_test = None
            y_test = None

        if num_cpus:
            params["n_jobs"] = num_cpus
        max_category_levels = params.pop("proc.max_category_levels", 100)
        enable_categorical = params.get("enable_categorical", False)
        if enable_categorical:
            """Skip one-hot-encoding and pass categoricals directly to XGBoost"""
            self._ohe = False
        else:
            """One-hot-encode categorical features"""
            self._ohe = True

        if verbosity <= 2:
            verbose = False
            log_period = None
        elif verbosity == 3:
            verbose = True
            log_period = 50
        else:
            verbose = True
            log_period = 1

        eval_set = {}
        prep_params = params.pop("prep_params", {})
        X = self.preprocess(X, y=y, is_train=True, prep_params=prep_params, max_category_levels=max_category_levels)
        num_rows_train = X.shape[0]

        # NOTE: xgb eval_metric param supports: default xgb metric(s), str or list(str) OR custom_metric generated by func_generator() in xgboost_utils
        # xgb does not support list(custom_metrics). Instead, use the CustomMetricCallback defined in xgb callbacks file
        if "eval_metric" not in params:
            eval_metric = self.get_eval_metric()
            if eval_metric is not None:
                params["eval_metric"] = eval_metric
                eval_metric_name = eval_metric.__name__ if not isinstance(eval_metric, str) else eval_metric

        if X_val is None:
            early_stopping_rounds = None
            eval_set = None
        else:
            X_val = self.preprocess(X_val, is_train=False)
            eval_set["val"] = (X_val, y_val)
            early_stopping_rounds = ag_params.get("early_stop", "adaptive")
            if isinstance(early_stopping_rounds, (str, tuple, list)):
                early_stopping_rounds = self._get_early_stopping_rounds(num_rows_train=num_rows_train, strategy=early_stopping_rounds)

        if generate_curves and eval_set is not None:
            scorers = ag_params.get("curve_metrics", [self.eval_metric])
            use_curve_metric_error = ag_params.get("use_error_for_curve_metrics", False)
            metric_names = [scorer.name for scorer in scorers]

            eval_set["train"] = (X, y)
            if X_test is not None:
                X_test = self.preprocess(X_test, is_train=False)
                eval_set["test"] = (X_test, y_test)

        if num_gpus != 0:
            if "device" not in params:
                # FIXME: figure out which GPUs are available to this model instead of hardcoding GPU 0.
                #  Need to update BaggedEnsembleModel
                params["device"] = "cuda:0"
        if "tree_method" not in params:
            params["tree_method"] = "hist"

        try_import_xgboost()
        from xgboost.callback import EvaluationMonitor

        from autogluon.tabular.models.xgboost.callbacks import CustomMetricCallback, EarlyStoppingCustom

        if eval_set is not None and "callbacks" not in params:
            callbacks = []
            if generate_curves:
                callbacks.append(CustomMetricCallback(scorers=scorers, eval_sets=eval_set, problem_type=self.problem_type, use_error=use_curve_metric_error))
            if log_period is not None:
                callbacks.append(EvaluationMonitor(period=log_period))

            callbacks.append(
                EarlyStoppingCustom(
                    early_stopping_rounds,
                    start_time=start_time,
                    time_limit=time_limit,
                    verbose=verbose,
                    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric
                    data_name="validation_0",  # forces val dataset rather than arbitrary last dataset
                )
            )
            params["callbacks"] = callbacks

        if eval_set is not None:
            # important that val dataset is listed first
            # (since validation_0 is specified in EarlyStoppingCustom callback)
            eval_set = [eval_set["val"]]

        from xgboost import XGBClassifier, XGBRegressor

        model_type = XGBClassifier if self.problem_type in PROBLEM_TYPES_CLASSIFICATION else XGBRegressor

        import warnings

        with warnings.catch_warnings():
            # FIXME: v1.1: Upgrade XGBoost to 2.0.1+ to avoid deprecation warnings from Pandas 2.1+ during XGBoost fit.
            warnings.simplefilter(action="ignore", category=FutureWarning)
            if params.get("device", "cpu") == "cuda:0":
                # verbosity=0 to hide UserWarning: Falling back to prediction using DMatrix due to mismatched devices.
                # TODO: Find a way to hide this warning without setting verbosity=0
                #  ref: https://github.com/dmlc/xgboost/issues/9791
                params["verbosity"] = 0
            self.model = model_type(**params)
            self.model.fit(X=X, y=y, eval_set=eval_set, verbose=False, sample_weight=sample_weight)

        if generate_curves:

            def filter(d, keys):
                # ensure only specified curve metrics are included
                return {key: d[key] for key in keys if key in d}

            eval_results = self.model.evals_result().copy()
            del eval_results["validation_0"]

            curves = {name: filter(metrics, metric_names) for name, metrics in eval_results.items()}
            self.save_learning_curves(metrics=metric_names, curves=curves)

        bst = self.model.get_booster()
        # TODO: Investigate speed-ups from GPU inference
        # bst.set_param({"predictor": "gpu_predictor"})

        if eval_set is not None:
            self.params_trained["n_estimators"] = bst.best_iteration + 1
        # Don't save the callback or eval_metric objects
        self.model.set_params(callbacks=None, eval_metric=None)
